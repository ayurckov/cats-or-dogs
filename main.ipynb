{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9723de80",
   "metadata": {},
   "source": [
    "<h3>Импорт</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e60ae8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision as tv\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63636972",
   "metadata": {},
   "source": [
    "<h3>Датасет</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fafb9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset2class(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_dir1:str, path_dir2:str):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.path_dir1 = path_dir1\n",
    "        self.path_dir2 = path_dir2\n",
    "        \n",
    "        # Keep only common image files and ensure they're actual files\n",
    "        allowed_ext = ('.jpg', '.jpeg', '.png', '.bmp')\n",
    "        self.dir1_list = sorted([f for f in os.listdir(path_dir1)\n",
    "                                 if f.lower().endswith(allowed_ext) and os.path.isfile(os.path.join(path_dir1, f))])\n",
    "        self.dir2_list = sorted([f for f in os.listdir(path_dir2)\n",
    "                                 if f.lower().endswith(allowed_ext) and os.path.isfile(os.path.join(path_dir2, f))])\n",
    "        \n",
    "        if len(self.dir1_list) == 0 or len(self.dir2_list) == 0:\n",
    "            raise RuntimeError(f\"No images found in {path_dir1} or {path_dir2} (checked extensions {allowed_ext})\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.dir1_list):\n",
    "            class_id = 0 \n",
    "            img_path = os.path.join(self.path_dir1, self.dir1_list[idx])\n",
    "        else: \n",
    "            class_id = 1\n",
    "            idx -= len(self.dir1_list)\n",
    "            img_path = os.path.join(self.path_dir2, self.dir2_list[idx])\n",
    "            \n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            # Fail fast with informative message instead of silent errors/hangs\n",
    "            raise FileNotFoundError(f\"Failed to read image: {img_path}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img/255.0\n",
    "        \n",
    "        img = cv2.resize(img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        img = img.transpose((2, 0, 1))\n",
    "        \n",
    "        t_img = torch.from_numpy(img)\n",
    "        t_class_id = torch.tensor(class_id)\n",
    "        \n",
    "        return {'img': t_img, 'label': t_class_id}\n",
    "         \n",
    "    def __len__(self):\n",
    "        return len(self.dir1_list) + len(self.dir2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e4296dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats_dir = './PetImages/Cat'\n",
    "train_dogs_dir = './PetImages/Dog'\n",
    "\n",
    "test_cats_dir = './PetImages/Cat'\n",
    "test_dogs_dir = './PetImages/Dog'\n",
    "\n",
    "train_ds_catsdogs = Dataset2class(train_cats_dir, train_dogs_dir)\n",
    "test_ds_catsdogs = Dataset2class(test_cats_dir, test_dogs_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af80ade",
   "metadata": {},
   "source": [
    "<h3>Data Loader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b29d2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds_catsdogs, shuffle=True, \n",
    "    batch_size=batch_size, num_workers=0, drop_last=True \n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_ds_catsdogs, shuffle=True, \n",
    "    batch_size=batch_size, num_workers=0, drop_last=False \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6bf0a",
   "metadata": {},
   "source": [
    "<h3>Architecture</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e16d9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сверточная нейронная сеть\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(3, 32, 3, stride=1, padding=0)\n",
    "        self.conv1 = nn.Conv2d(32, 32, 3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=1, padding=0)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=1, padding=0)\n",
    "        \n",
    "        self.adaptivepool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        # flattened features == number of channels after convs (32)\n",
    "        self.linear1 = nn.Linear(32, 10)\n",
    "        self.linear2 = nn.Linear(10, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv0(x)\n",
    "        out = self.act(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.conv1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.act(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.act(out)\n",
    "        \n",
    "        out = self.adaptivepool(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.linear1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0654b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd861b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (act): LeakyReLU(negative_slope=0.2)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (adaptivepool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear1): Linear(in_features=32, out_features=10, bias=True)\n",
       "  (linear2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "538c5504",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_loader:\n",
    "    img = sample['img']\n",
    "    label = sample['label']\n",
    "    model(img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "214ab75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 64, 64])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
