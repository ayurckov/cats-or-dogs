{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9723de80",
   "metadata": {},
   "source": [
    "<h3>Import</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e60ae8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision as tv\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63636972",
   "metadata": {},
   "source": [
    "<h3>Data set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5fafb9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset2class(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_dir1:str, path_dir2:str, validate_images:bool=False, validate_sample_size:int=100, remove_bad:bool=False, bad_dir: str = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.path_dir1 = path_dir1\n",
    "        self.path_dir2 = path_dir2\n",
    "        self.remove_bad = remove_bad\n",
    "        self.bad_dir = bad_dir\n",
    "        \n",
    "        allowed_ext = ('.jpg', '.jpeg', '.png', '.bmp')\n",
    "        # Quick filter by extension and non-zero file size (fast)\n",
    "        dir1_files = sorted([f for f in os.listdir(path_dir1)\n",
    "                             if f.lower().endswith(allowed_ext) \n",
    "                             and os.path.isfile(os.path.join(path_dir1, f))\n",
    "                             and os.path.getsize(os.path.join(path_dir1, f)) > 0])\n",
    "        dir2_files = sorted([f for f in os.listdir(path_dir2)\n",
    "                             if f.lower().endswith(allowed_ext) \n",
    "                             and os.path.isfile(os.path.join(path_dir2, f))\n",
    "                             and os.path.getsize(os.path.join(path_dir2, f)) > 0])\n",
    "\n",
    "        # If full validation requested, try reading images (slower)\n",
    "        def _filter_valid_files(dir_path, files):\n",
    "            valid, removed = [], []\n",
    "            for fn in files:\n",
    "                p = os.path.join(dir_path, fn)\n",
    "                try:\n",
    "                    img = cv2.imread(p, cv2.IMREAD_COLOR)\n",
    "                    if img is None or img.size == 0:\n",
    "                        removed.append(fn)\n",
    "                    else:\n",
    "                        valid.append(fn)\n",
    "                except Exception:\n",
    "                    removed.append(fn)\n",
    "            return valid, removed\n",
    "\n",
    "        if validate_images:\n",
    "            self.dir1_list, removed1 = _filter_valid_files(path_dir1, dir1_files)\n",
    "            self.dir2_list, removed2 = _filter_valid_files(path_dir2, dir2_files)\n",
    "            self.removed_files_summary = {path_dir1: removed1, path_dir2: removed2}\n",
    "            self.validated = True\n",
    "        else:\n",
    "            self.dir1_list, self.dir2_list = dir1_files, dir2_files\n",
    "            self.removed_files_summary = {path_dir1: [], path_dir2: []}\n",
    "            self.validated = False\n",
    "        \n",
    "        # runtime-removed files recorded here\n",
    "        self.runtime_removed = {path_dir1: [], path_dir2: []}\n",
    "        \n",
    "        if len(self.dir1_list) == 0 or len(self.dir2_list) == 0:\n",
    "            raise RuntimeError(f\"No valid images found in {path_dir1} or {path_dir2} (checked extensions {allowed_ext})\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Defensive normalization: if idx is outside current dataset (due to prior removals),\n",
    "        # wrap it into the current length so DataLoader's precomputed indices won't break.\n",
    "        total_len = len(self)\n",
    "        if total_len == 0:\n",
    "            raise RuntimeError(\"Dataset is empty (all files removed or invalid).\")\n",
    "        if idx < 0:\n",
    "            idx = idx % total_len\n",
    "        if idx >= total_len:\n",
    "            idx = idx % total_len\n",
    "\n",
    "        # Try up to total_len times to find a readable image (avoids infinite loop)\n",
    "        attempts = 0\n",
    "        max_attempts = total_len\n",
    "        while attempts < max_attempts:\n",
    "            total_len = len(self)\n",
    "            if total_len == 0:\n",
    "                raise RuntimeError(\"Dataset is empty (all files removed or invalid).\")\n",
    "\n",
    "            # Normalize index each loop because lists may have shrunk\n",
    "            idx = idx % total_len\n",
    "\n",
    "            if idx < len(self.dir1_list):\n",
    "                class_id = 0\n",
    "                local_idx = idx\n",
    "                filename = self.dir1_list[local_idx]\n",
    "                img_path = os.path.join(self.path_dir1, filename)\n",
    "                list_ref = self.dir1_list\n",
    "                list_key = self.path_dir1\n",
    "            else:\n",
    "                class_id = 1\n",
    "                local_idx = idx - len(self.dir1_list)\n",
    "                # If local_idx somehow exceeds second list (possible after removals), normalize and retry\n",
    "                if local_idx >= len(self.dir2_list):\n",
    "                    idx = idx % total_len\n",
    "                    attempts += 1\n",
    "                    continue\n",
    "                filename = self.dir2_list[local_idx]\n",
    "                img_path = os.path.join(self.path_dir2, filename)\n",
    "                list_ref = self.dir2_list\n",
    "                list_key = self.path_dir2\n",
    "\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "            if img is None or img.size == 0:\n",
    "                # record and optionally move the bad file; remove it from list and retry same idx\n",
    "                self.runtime_removed[list_key].append(filename)\n",
    "                try:\n",
    "                    if self.remove_bad and self.bad_dir:\n",
    "                        os.makedirs(self.bad_dir, exist_ok=True)\n",
    "                        dest = os.path.join(self.bad_dir, f\"{list_key.replace('/','_').replace(':','')}_{filename}\")\n",
    "                        try:\n",
    "                            shutil.move(img_path, dest)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                except Exception:\n",
    "                    pass\n",
    "                del list_ref[local_idx]\n",
    "                attempts += 1\n",
    "                continue\n",
    "\n",
    "            # good image — preprocess and return\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "            img = cv2.resize(img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "            img = img.transpose((2, 0, 1))\n",
    "            \n",
    "            t_img = torch.from_numpy(img)\n",
    "            t_class_id = torch.tensor(class_id, dtype=torch.long)\n",
    "            \n",
    "            return {'img': t_img, 'label': t_class_id}\n",
    "\n",
    "        # if we reach here, many attempts failed => dataset likely corrupted\n",
    "        raise RuntimeError(\"Unable to fetch a readable image after multiple attempts; dataset may be corrupted.\")\n",
    "         \n",
    "    def __len__(self):\n",
    "        return len(self.dir1_list) + len(self.dir2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e4296dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 24997 Test size: 24997\n"
     ]
    }
   ],
   "source": [
    "train_cats_dir = './PetImages/Cat'\n",
    "train_dogs_dir = './PetImages/Dog'\n",
    "\n",
    "test_cats_dir = './PetImages/Cat'\n",
    "test_dogs_dir = './PetImages/Dog'\n",
    "\n",
    "# Enable remove_bad and set a bad_dir so problematic files are moved and won't stall later\n",
    "train_ds_catsdogs = Dataset2class(train_cats_dir, train_dogs_dir, validate_images=False, remove_bad=True, bad_dir='./bad_images')\n",
    "test_ds_catsdogs = Dataset2class(test_cats_dir, test_dogs_dir, validate_images=False, remove_bad=True, bad_dir='./bad_images')\n",
    "\n",
    "print(\"Train size:\", len(train_ds_catsdogs), \"Test size:\", len(test_ds_catsdogs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af80ade",
   "metadata": {},
   "source": [
    "<h3>Data Loader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b29d2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds_catsdogs, shuffle=True, \n",
    "    batch_size=batch_size, num_workers=0, drop_last=True \n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_ds_catsdogs, shuffle=True, \n",
    "    batch_size=batch_size, num_workers=0, drop_last=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "55970731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: img torch.Size([16, 3, 64, 64]), labels torch.Size([16])\n",
      "Batch 2: img torch.Size([16, 3, 64, 64]), labels torch.Size([16])\n",
      "Batch 3: img torch.Size([16, 3, 64, 64]), labels torch.Size([16])\n",
      "Batch 4: img torch.Size([16, 3, 64, 64]), labels torch.Size([16])\n",
      "Batch 5: img torch.Size([16, 3, 64, 64]), labels torch.Size([16])\n",
      "Elapsed (s): 0.1394639015197754\n",
      "Runtime removed (train): {'./PetImages/Cat': [], './PetImages/Dog': []}\n",
      "Runtime removed (test): {'./PetImages/Cat': [], './PetImages/Dog': []}\n"
     ]
    }
   ],
   "source": [
    "# Quick dataloader sanity check: iterate a few batches and ensure it doesn't stall; then print runtime-removed\n",
    "import time\n",
    "def check_loader(loader, max_batches=5):\n",
    "    start = time.time()\n",
    "    for i, sample in enumerate(loader):\n",
    "        print(f\"Batch {i+1}: img {sample['img'].shape}, labels {sample['label'].shape}\")\n",
    "        if i+1 >= max_batches:\n",
    "            break\n",
    "    print(\"Elapsed (s):\", time.time() - start)\n",
    "\n",
    "check_loader(train_loader, max_batches=5)\n",
    "\n",
    "# show runtime-removed files (if any)\n",
    "print(\"Runtime removed (train):\", train_ds_catsdogs.runtime_removed)\n",
    "print(\"Runtime removed (test):\", test_ds_catsdogs.runtime_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6bf0a",
   "metadata": {},
   "source": [
    "<h3>Architecture</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e16d9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сверточная нейронная сеть\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(3, 32, 3, stride=1, padding=0)\n",
    "        self.conv1 = nn.Conv2d(32, 32, 3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=1, padding=0)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=1, padding=0)\n",
    "        \n",
    "        self.adaptivepool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        # flattened features == number of channels after convs (32)\n",
    "        self.linear1 = nn.Linear(32, 10)\n",
    "        self.linear2 = nn.Linear(10, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv0(x)\n",
    "        out = self.act(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.conv1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.act(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.act(out)\n",
    "        \n",
    "        out = self.adaptivepool(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.linear1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d020076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a0654b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd861b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (act): LeakyReLU(negative_slope=0.2)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (adaptivepool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear1): Linear(in_features=32, out_features=10, bias=True)\n",
       "  (linear2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "acec1cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38240"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "538c5504",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_loader:\n",
    "    img = sample['img']\n",
    "    label = sample['label']\n",
    "    model(img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "214ab75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 64, 64])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474281d2",
   "metadata": {},
   "source": [
    "<h3>Optimizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "497181f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b74a1032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, label):\n",
    "    # pred: logits, label: class indices (LongTensor)\n",
    "    pred_cls = pred.detach().argmax(dim=1)\n",
    "    true_cls = label.detach().argmax(dim=1) if label.dim() > 1 else label.detach()\n",
    "    return (pred_cls == true_cls).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47160d",
   "metadata": {},
   "source": [
    "<h3>Learning cycle</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "665560b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1562/1562 [01:06<00:00, 23.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6830153596729384 0.5513364276568502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1562/1562 [01:05<00:00, 23.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6401907743816949 0.6321222791293214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1562/1562 [01:06<00:00, 23.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6167383771699766 0.662291933418694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1562/1562 [01:05<00:00, 23.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.582407337297398 0.6954225352112676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1562/1562 [01:05<00:00, 23.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5552855204032416 0.7191101152368758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1562/1562 [01:05<00:00, 23.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5357778915865931 0.7324743918053778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1562/1562 [01:10<00:00, 22.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5192595571279526 0.7464788732394366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1562/1562 [03:05<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5039716493362196 0.7554417413572343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1562/1562 [01:13<00:00, 21.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49149217350687474 0.7623239436619719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1562/1562 [01:10<00:00, 22.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.478084067700767 0.7732874519846351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_val = 0.0\n",
    "    acc_val = 0.0\n",
    "    skipped_batches = 0\n",
    "    for sample in (pbar := tqdm(train_loader)):\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            img, label = sample['img'], sample['label']\n",
    "            pred = model(img)\n",
    "            \n",
    "            loss = loss_fn(pred, label)  # CrossEntropyLoss expects class indices\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_item = loss.item()\n",
    "            loss_val += loss_item\n",
    "            \n",
    "            accuracy_val = accuracy(pred, label)\n",
    "            acc_val += accuracy_val\n",
    "        except Exception as e:\n",
    "            # Log and skip problematic batch to avoid endless stall\n",
    "            skipped_batches += 1\n",
    "            print(f\"Skipping batch due to error: {e}\")\n",
    "            continue\n",
    "        \n",
    "    pbar.set_description(f\"Epoch {epoch+1}/{epochs} Loss: {loss_val/len(train_loader):.4f} Acc: {acc_val/len(train_loader):.4f} Skipped: {skipped_batches}\")\n",
    "    print(loss_val/len(train_loader), acc_val/len(train_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
